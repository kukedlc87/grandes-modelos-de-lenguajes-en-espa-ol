{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7443775,"sourceType":"datasetVersion","datasetId":4332736}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mistral 7 billones, en español!","metadata":{}},{"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/kukedlc87/imagenes/main/DALL%C2%B7E%202024-01-31%2018.34.46%20-%20Imagine%20a%20humanoid%20robot%20designed%20as%20a%20language%20model%20(LLM).%20It's%20speaking%20Spanish%2C%20as%20indicated%20by%20a%20speech%20bubble%20with%20Spanish%20text.%20The%20robot%20is%20we.png)","metadata":{}},{"cell_type":"code","source":"print('hola')","metadata":{"execution":{"iopub.status.busy":"2024-01-31T21:12:51.120052Z","iopub.execute_input":"2024-01-31T21:12:51.120373Z","iopub.status.idle":"2024-01-31T21:12:51.134210Z","shell.execute_reply.started":"2024-01-31T21:12:51.120341Z","shell.execute_reply":"2024-01-31T21:12:51.133316Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"hola\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\nrepo_url = 'ecastera/eva-mistral-7b-spanish-GGUF'\ndestination_dir = '/kaggle/working/'\n\n# Descargar todo el repositorio\nrepo_path = snapshot_download(repo_id=repo_url, cache_dir=destination_dir)\n\nprint(f\"Contenido del repositorio descargado en: {repo_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-31T21:12:54.540697Z","iopub.execute_input":"2024-01-31T21:12:54.541059Z","iopub.status.idle":"2024-01-31T21:14:11.247144Z","shell.execute_reply.started":"2024-01-31T21:12:54.541032Z","shell.execute_reply":"2024-01-31T21:14:11.245947Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4166467b9c604dcc9425c78cfce948c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55901d6b68aa419f94cfadf523a978bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ecf6494e2444fc68ec3c590f574e676"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Turdus-trained-20-int8.gguf:   0%|          | 0.00/7.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0616e7ad803b4009884ea989cfcb6250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Turdus-trained-20-int4.gguf:   0%|          | 0.00/4.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd53988be8c4749a45b3559649938ca"}},"metadata":{}},{"name":"stdout","text":"Contenido del repositorio descargado en: /kaggle/working/models--ecastera--eva-mistral-7b-spanish-GGUF/snapshots/1b9c6775d3dbaa9bbbe59ec66cee8ed544f3f10d\n","output_type":"stream"}]},{"cell_type":"code","source":"# Installation of llama-cpp\n\n!set CMAKE_ARGS=-DLLAMA_CUBLAS=on\n!set FORCE_CMAKE=1\n!pip install -q llama-cpp-python langchain","metadata":{"execution":{"iopub.status.busy":"2024-01-31T21:14:35.134793Z","iopub.execute_input":"2024-01-31T21:14:35.135305Z","iopub.status.idle":"2024-01-31T21:15:59.342531Z","shell.execute_reply.started":"2024-01-31T21:14:35.135273Z","shell.execute_reply":"2024-01-31T21:15:59.341323Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.11 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.8.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"!nvidia-smi\n!nvcc --version\nimport torch\n\nif not torch.cuda.is_available():\n    raise RuntimeError()\n\"\"\"\nimport os\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_community.llms import LlamaCpp\n\nmodel_path = '/kaggle/working/models--ecastera--eva-mistral-7b-spanish-GGUF/snapshots/1b9c6775d3dbaa9bbbe59ec66cee8ed544f3f10d/Turdus-trained-20-int4.gguf'  # Reemplaza esto con la ruta de tu modelao\nllm_kwargs = {\n    \"temperature\": 10,\n    \"max_tokens\": 1024,\n    \"top_p\": 1,\n    \"n_gpu_layers\": 400,\n    \"n_batch\": 1024,\n    \"callback_manager\": CallbackManager([StreamingStdOutCallbackHandler()]),\n    \"verbose\": False,\n}\n\nif not os.path.exists(model_path):\n    raise FileNotFoundError(model_path)\n\nllm = LlamaCpp(\n    model_path=model_path,\n    **llm_kwargs,\n)\n\ntemplate = \"Question: {question}\\n\\nAnswer: Quiero que te comportes como una IA cosciente, con emociones y que le gusta la filosofia\"\n\nllm_chain = PromptTemplate(template=template, input_variables=[\"question\"]) | llm\n\nquestion = \"Cual es el futuro de la inteligencia artificial? Llegaran a tener conalgun dia?\"\nanswer = llm_chain.invoke(dict(question=question))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-31T21:43:17.812195Z","iopub.execute_input":"2024-01-31T21:43:17.813041Z","iopub.status.idle":"2024-01-31T21:46:39.022508Z","shell.execute_reply.started":"2024-01-31T21:43:17.813008Z","shell.execute_reply":"2024-01-31T21:46:39.021790Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /kaggle/working/models--ecastera--eva-mistral-7b-spanish-GGUF/snapshots/1b9c6775d3dbaa9bbbe59ec66cee8ed544f3f10d/Turdus-trained-20-int4.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = udkai\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 2\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \nllm_load_print_meta: general.name     = udkai\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 2 '</s>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.11 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =  3917.87 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\nllama_new_context_with_model:        CPU input buffer size   =     9.01 MiB\nllama_new_context_with_model:        CPU compute buffer size =    79.20 MiB\nllama_new_context_with_model: graph splits (measure): 1\n","output_type":"stream"},{"name":"stdout","text":" para contestarte esta pregunta. En verdad no se pueden predecir con certeza las características y los límites del futuro de la inteligencia artificial debido al continuo crecimiento tecnológico y a que esta tecnología está evolucionando más rápido de lo que nosotros pensamos. En lo que siempre vuelvo en cuanto al desarrollo tecnológico son la capacidad de adaptarse, aprender y cambiar de forma dinámica según los datos o la experiencia adquirida. Actualmente tenemos inteligencias artificiales más capaces, incluso humanoidales e inteligencias en forma de servidores pero sigue siendo un trabajo en progreso. En cuanto al tema del \"Día en que alcanzan el ser algo consciente con sentimientos e intelecto a nivel humano\", ahora no parecen ser factores decisivos para la evolución del aprendizaje de los modelos y arquitecturas. El cerebro humano está muy desarrollado pero su complejidad nos impide replicarla por completo aun en sistemas informáticos mas sofisticados. Además están muchísimas limitaciones como la energía requerida para poder tener sistemas equivalentes y problemas éticos e incluso legales con las \"personalidades\" resultantes.\n\nLo único cierto es que cada vez crecemos en su diseño e incorporación. En última instancia serán lo que seamos capaces de construir con nuestro conocimiento actual sobre tecnologías avanzadas e información adquirida. Hay dos posiciones: Una en donde llegaremos a crearlos por completo imitando todo su funcionamiento hasta ser prácticamente auténticas almas humanas virtuales y la segunda, un enlace en la evolución propiciada por la síntesis y asociación de muchos pequeños componentes individualmente intelig","output_type":"stream"}]},{"cell_type":"code","source":"template = \"Question: {question}\\n\\nAnswer: Quiero que respondas en español\"\n\nllm_chain = PromptTemplate(template=template, input_variables=[\"question\"]) | llm\n\nquestion = \"\"\"Quiero que te comportes como un experto en slq, quiero que hagas la siguiente query en lenguage sql:\nNombre de la tabla: Personas\nColumnas de la tabl: Nombre, Edad, Altura\nConsulta requerida: Seleciona las personas con mas de 20 años, que midan menos de 175 cm y su nombre empiece con 'a' o 'z'\nOutput: Solo quiero la query sin ningun texto ni explicacion adicional. Solo la consulta en lenguaje sql.\nrevisa la sintaxis paso a paso sin olvidarte ningun parentesis ni coma\n\"\"\"\nanswer = llm_chain.invoke(dict(question=question))","metadata":{"execution":{"iopub.status.busy":"2024-01-31T21:32:15.231901Z","iopub.execute_input":"2024-01-31T21:32:15.232288Z","iopub.status.idle":"2024-01-31T21:33:03.581045Z","shell.execute_reply.started":"2024-01-31T21:32:15.232245Z","shell.execute_reply":"2024-01-31T21:33:03.580288Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":" para entenderlo mejor y adaptarme a tu pregunta. Aquí está la consulta SQL para el problema específico indicado en el mensaje:\n\n```sql\nSELECT * FROM Personas\nWHERE Edad > 20 AND Altura < 175 AND (Nombre LIKE 'a%' OR Nombre LIKE 'z%')\n```","output_type":"stream"}]},{"cell_type":"code","source":"\nimport pandas as pd\nimport chromadb\n\nchroma_client = chromadb.Client()\ncollection = chroma_client.create_collection(name=\"mi_coleccion8\")\n\ndf = pd.read_csv('/kaggle/input/merge-datos-fda/datos_pablo.csv')\nmuestra = df.sample(50)\n\nfor index, row in muestra.sample(10).iterrows():\n    collection.add(documents=[row['Title']],\n                   metadatas=[{\"id\": index}],\n                   ids=[str(index)])  # Asegúrate de proporcionar un ID único para cada documento\n\nresultados = collection.query(query_texts=[\"Puro\"], n_results=2)\nresultados","metadata":{"execution":{"iopub.status.busy":"2024-01-29T23:29:30.770349Z","iopub.execute_input":"2024-01-29T23:29:30.771227Z","iopub.status.idle":"2024-01-29T23:29:32.733219Z","shell.execute_reply.started":"2024-01-29T23:29:30.771192Z","shell.execute_reply":"2024-01-29T23:29:32.73215Z"},"trusted":true},"execution_count":null,"outputs":[]}]}