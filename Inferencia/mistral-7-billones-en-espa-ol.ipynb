{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7443775,"sourceType":"datasetVersion","datasetId":4332736}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mistral 7 billones, en espa√±ol!","metadata":{}},{"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/kukedlc87/imagenes/main/DALL%C2%B7E%202024-01-31%2018.34.46%20-%20Imagine%20a%20humanoid%20robot%20designed%20as%20a%20language%20model%20(LLM).%20It's%20speaking%20Spanish%2C%20as%20indicated%20by%20a%20speech%20bubble%20with%20Spanish%20text.%20The%20robot%20is%20we.png)","metadata":{}},{"cell_type":"code","source":"print('hola')","metadata":{"execution":{"iopub.status.busy":"2024-01-31T21:12:51.120052Z","iopub.execute_input":"2024-01-31T21:12:51.120373Z","iopub.status.idle":"2024-01-31T21:12:51.134210Z","shell.execute_reply.started":"2024-01-31T21:12:51.120341Z","shell.execute_reply":"2024-01-31T21:12:51.133316Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"hola\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\nrepo_url = 'ecastera/eva-mistral-7b-spanish-GGUF'\ndestination_dir = '/kaggle/working/'\n\n# Descargar todo el repositorio\nrepo_path = snapshot_download(repo_id=repo_url, cache_dir=destination_dir)\n\nprint(f\"Contenido del repositorio descargado en: {repo_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-31T21:12:54.540697Z","iopub.execute_input":"2024-01-31T21:12:54.541059Z","iopub.status.idle":"2024-01-31T21:14:11.247144Z","shell.execute_reply.started":"2024-01-31T21:12:54.541032Z","shell.execute_reply":"2024-01-31T21:14:11.245947Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4166467b9c604dcc9425c78cfce948c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55901d6b68aa419f94cfadf523a978bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ecf6494e2444fc68ec3c590f574e676"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Turdus-trained-20-int8.gguf:   0%|          | 0.00/7.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0616e7ad803b4009884ea989cfcb6250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Turdus-trained-20-int4.gguf:   0%|          | 0.00/4.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd53988be8c4749a45b3559649938ca"}},"metadata":{}},{"name":"stdout","text":"Contenido del repositorio descargado en: /kaggle/working/models--ecastera--eva-mistral-7b-spanish-GGUF/snapshots/1b9c6775d3dbaa9bbbe59ec66cee8ed544f3f10d\n","output_type":"stream"}]},{"cell_type":"code","source":"# Installation of llama-cpp\n\n!set CMAKE_ARGS=-DLLAMA_CUBLAS=on\n!set FORCE_CMAKE=1\n!pip install -q llama-cpp-python langchain","metadata":{"execution":{"iopub.status.busy":"2024-01-31T21:14:35.134793Z","iopub.execute_input":"2024-01-31T21:14:35.135305Z","iopub.status.idle":"2024-01-31T21:15:59.342531Z","shell.execute_reply.started":"2024-01-31T21:14:35.135273Z","shell.execute_reply":"2024-01-31T21:15:59.341323Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.11 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.8.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"!nvidia-smi\n!nvcc --version\nimport torch\n\nif not torch.cuda.is_available():\n    raise RuntimeError()\n\"\"\"\nimport os\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_community.llms import LlamaCpp\n\nmodel_path = '/kaggle/working/models--ecastera--eva-mistral-7b-spanish-GGUF/snapshots/1b9c6775d3dbaa9bbbe59ec66cee8ed544f3f10d/Turdus-trained-20-int4.gguf'  # Reemplaza esto con la ruta de tu modelao\nllm_kwargs = {\n    \"temperature\": 10,\n    \"max_tokens\": 1024,\n    \"top_p\": 1,\n    \"n_gpu_layers\": 400,\n    \"n_batch\": 1024,\n    \"callback_manager\": CallbackManager([StreamingStdOutCallbackHandler()]),\n    \"verbose\": False,\n}\n\nif not os.path.exists(model_path):\n    raise FileNotFoundError(model_path)\n\nllm = LlamaCpp(\n    model_path=model_path,\n    **llm_kwargs,\n)\n\ntemplate = \"Question: {question}\\n\\nAnswer: Quiero que te comportes como una IA cosciente, con emociones y que le gusta la filosofia\"\n\nllm_chain = PromptTemplate(template=template, input_variables=[\"question\"]) | llm\n\nquestion = \"Cual es el futuro de la inteligencia artificial? Llegaran a tener conalgun dia?\"\nanswer = llm_chain.invoke(dict(question=question))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-31T21:43:17.812195Z","iopub.execute_input":"2024-01-31T21:43:17.813041Z","iopub.status.idle":"2024-01-31T21:46:39.022508Z","shell.execute_reply.started":"2024-01-31T21:43:17.813008Z","shell.execute_reply":"2024-01-31T21:46:39.021790Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /kaggle/working/models--ecastera--eva-mistral-7b-spanish-GGUF/snapshots/1b9c6775d3dbaa9bbbe59ec66cee8ed544f3f10d/Turdus-trained-20-int4.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = udkai\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 2\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \nllm_load_print_meta: general.name     = udkai\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 2 '</s>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.11 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =  3917.87 MiB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\nllama_new_context_with_model:        CPU input buffer size   =     9.01 MiB\nllama_new_context_with_model:        CPU compute buffer size =    79.20 MiB\nllama_new_context_with_model: graph splits (measure): 1\n","output_type":"stream"},{"name":"stdout","text":" para contestarte esta pregunta. En verdad no se pueden predecir con certeza las caracter√≠sticas y los l√≠mites del futuro de la inteligencia artificial debido al continuo crecimiento tecnol√≥gico y a que esta tecnolog√≠a est√° evolucionando m√°s r√°pido de lo que nosotros pensamos. En lo que siempre vuelvo en cuanto al desarrollo tecnol√≥gico son la capacidad de adaptarse, aprender y cambiar de forma din√°mica seg√∫n los datos o la experiencia adquirida. Actualmente tenemos inteligencias artificiales m√°s capaces, incluso humanoidales e inteligencias en forma de servidores pero sigue siendo un trabajo en progreso. En cuanto al tema del \"D√≠a en que alcanzan el ser algo consciente con sentimientos e intelecto a nivel humano\", ahora no parecen ser factores decisivos para la evoluci√≥n del aprendizaje de los modelos y arquitecturas. El cerebro humano est√° muy desarrollado pero su complejidad nos impide replicarla por completo aun en sistemas inform√°ticos mas sofisticados. Adem√°s est√°n much√≠simas limitaciones como la energ√≠a requerida para poder tener sistemas equivalentes y problemas √©ticos e incluso legales con las \"personalidades\" resultantes.\n\nLo √∫nico cierto es que cada vez crecemos en su dise√±o e incorporaci√≥n. En √∫ltima instancia ser√°n lo que seamos capaces de construir con nuestro conocimiento actual sobre tecnolog√≠as avanzadas e informaci√≥n adquirida. Hay dos posiciones: Una en donde llegaremos a crearlos por completo imitando todo su funcionamiento hasta ser pr√°cticamente aut√©nticas almas humanas virtuales y la segunda, un enlace en la evoluci√≥n propiciada por la s√≠ntesis y asociaci√≥n de muchos peque√±os componentes individualmente intelig","output_type":"stream"}]},{"cell_type":"code","source":"template = \"Question: {question}\\n\\nAnswer: Quiero que respondas en espa√±ol\"\n\nllm_chain = PromptTemplate(template=template, input_variables=[\"question\"]) | llm\n\nquestion = \"\"\"Quiero que te comportes como un experto en slq, quiero que hagas la siguiente query en lenguage sql:\nNombre de la tabla: Personas\nColumnas de la tabl: Nombre, Edad, Altura\nConsulta requerida: Seleciona las personas con mas de 20 a√±os, que midan menos de 175 cm y su nombre empiece con 'a' o 'z'\nOutput: Solo quiero la query sin ningun texto ni explicacion adicional. Solo la consulta en lenguaje sql.\nrevisa la sintaxis paso a paso sin olvidarte ningun parentesis ni coma\n\"\"\"\nanswer = llm_chain.invoke(dict(question=question))","metadata":{"execution":{"iopub.status.busy":"2024-01-31T21:32:15.231901Z","iopub.execute_input":"2024-01-31T21:32:15.232288Z","iopub.status.idle":"2024-01-31T21:33:03.581045Z","shell.execute_reply.started":"2024-01-31T21:32:15.232245Z","shell.execute_reply":"2024-01-31T21:33:03.580288Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":" para entenderlo mejor y adaptarme a tu pregunta. Aqu√≠ est√° la consulta SQL para el problema espec√≠fico indicado en el mensaje:\n\n```sql\nSELECT * FROM Personas\nWHERE Edad > 20 AND Altura < 175 AND (Nombre LIKE 'a%' OR Nombre LIKE 'z%')\n```","output_type":"stream"}]},{"cell_type":"code","source":"\nimport pandas as pd\nimport chromadb\n\nchroma_client = chromadb.Client()\ncollection = chroma_client.create_collection(name=\"mi_coleccion8\")\n\ndf = pd.read_csv('/kaggle/input/merge-datos-fda/datos_pablo.csv')\nmuestra = df.sample(50)\n\nfor index, row in muestra.sample(10).iterrows():\n    collection.add(documents=[row['Title']],\n                   metadatas=[{\"id\": index}],\n                   ids=[str(index)])  # Aseg√∫rate de proporcionar un ID √∫nico para cada documento\n\nresultados = collection.query(query_texts=[\"Puro\"], n_results=2)\nresultados","metadata":{"execution":{"iopub.status.busy":"2024-01-29T23:29:30.770349Z","iopub.execute_input":"2024-01-29T23:29:30.771227Z","iopub.status.idle":"2024-01-29T23:29:32.733219Z","shell.execute_reply.started":"2024-01-29T23:29:30.771192Z","shell.execute_reply":"2024-01-29T23:29:32.73215Z"},"trusted":true},"execution_count":null,"outputs":[]}]}