{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural Beagle 7b on Kaggle with LLama-cpp","metadata":{}},{"cell_type":"markdown","source":"![](https://i.imgur.com/89ZAKcn.png)","metadata":{}},{"cell_type":"markdown","source":"# Descarga del modelo cuantizado desde huggingface","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\n\nrepo_url = 'TheBloke/NeuralBeagle14-7B-GGUF'\nmodel_filename = 'neuralbeagle14-7b.Q4_0.gguf'\n\ndestination_dir = '/kaggle/working/'\n\nmodel_path = hf_hub_download(repo_url, filename=model_filename, cache_dir=destination_dir)\n\nprint(f\"Modelo descargado en(Download in this path): {model_path}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Aca en teoria se habilita el proceso de cuda (gpu) pero la inferencia se realzia con CPU Be cause? I don´t know","metadata":{}},{"cell_type":"code","source":"# Installation of llama-cpp\n\n!set CMAKE_ARGS=-DLLAMA_CUBLAS=on\n!set FORCE_CMAKE=1\n!pip install -v llama-cpp-python","metadata":{"execution":{"iopub.status.busy":"2024-01-29T20:55:05.072050Z","iopub.execute_input":"2024-01-29T20:55:05.072896Z","iopub.status.idle":"2024-01-29T20:56:20.805654Z","shell.execute_reply.started":"2024-01-29T20:55:05.072863Z","shell.execute_reply":"2024-01-29T20:56:20.803998Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using pip 23.2.1 from /opt/conda/lib/python3.10/site-packages/pip (python 3.10)\nCollecting llama-cpp-python\n  Downloading llama_cpp_python-0.2.36.tar.gz (10.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Running command pip subprocess to install build dependencies\n  Collecting scikit-build-core[pyproject]>=0.5.1\n    Obtaining dependency information for scikit-build-core[pyproject]>=0.5.1 from https://files.pythonhosted.org/packages/0c/5b/73dc7944ef0fdbe97626b40525f1f9ca2547d7c5229b358d45357ff62209/scikit_build_core-0.8.0-py3-none-any.whl.metadata\n    Downloading scikit_build_core-0.8.0-py3-none-any.whl.metadata (19 kB)\n  Collecting exceptiongroup (from scikit-build-core[pyproject]>=0.5.1)\n    Obtaining dependency information for exceptiongroup from https://files.pythonhosted.org/packages/b8/9a/5028fd52db10e600f1c4674441b968cf2ea4959085bfb5b99fb1250e5f68/exceptiongroup-1.2.0-py3-none-any.whl.metadata\n    Downloading exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n  Collecting packaging>=20.9 (from scikit-build-core[pyproject]>=0.5.1)\n    Obtaining dependency information for packaging>=20.9 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\n    Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n  Collecting tomli>=1.1 (from scikit-build-core[pyproject]>=0.5.1)\n    Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n  Collecting pathspec>=0.10.1 (from scikit-build-core[pyproject]>=0.5.1)\n    Obtaining dependency information for pathspec>=0.10.1 from https://files.pythonhosted.org/packages/cc/20/ff623b09d963f88bfde16306a54e12ee5ea43e9b597108672ff3a408aad6/pathspec-0.12.1-py3-none-any.whl.metadata\n    Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n  Collecting pyproject-metadata>=0.5 (from scikit-build-core[pyproject]>=0.5.1)\n    Downloading pyproject_metadata-0.7.1-py3-none-any.whl (7.4 kB)\n  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n  \u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m51.2/53.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n  \u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n  \u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n  Downloading scikit_build_core-0.8.0-py3-none-any.whl (139 kB)\n  \u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/139.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m133.1/139.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n  \u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n  \u001b[?25hInstalling collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core, pyproject-metadata\n  \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n  cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n  cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n  dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n  cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n  cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n  cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\n  cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\n  dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\n  dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\n  dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n  dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\n  dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\n  dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n  google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\n  jupyterlab 4.0.10 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n  jupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n  libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n  momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n  pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\n  pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\n  raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\n  raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\n  spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n  ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n  \u001b[0mSuccessfully installed exceptiongroup-1.2.0 packaging-23.2 pathspec-0.12.1 pyproject-metadata-0.7.1 scikit-build-core-0.8.0 tomli-2.0.1\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Running command Getting requirements to build wheel\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Running command pip subprocess to install backend dependencies\n  Collecting ninja>=1.5\n    Obtaining dependency information for ninja>=1.5 from https://files.pythonhosted.org/packages/6d/92/8d7aebd4430ab5ff65df2bfee6d5745f95c004284db2d8ca76dcbfd9de47/ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata\n    Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n  \u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/307.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n  \u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/307.2 kB\u001b[0m \u001b[31m794.3 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/307.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n  \u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m286.7/307.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n  \u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n  \u001b[?25hInstalling collected packages: ninja\n  Successfully installed ninja-1.11.1.1\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Running command Preparing metadata (pyproject.toml)\n  \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.8.0\u001b[0m using \u001b[94mCMake 3.22.1\u001b[0m \u001b[91m(metadata_wheel)\u001b[0m\u001b[0m\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.5.0)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.24.3)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Running command Building wheel for llama-cpp-python (pyproject.toml)\n  \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.8.0\u001b[0m using \u001b[94mCMake 3.22.1\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n  \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n  2024-01-29 20:55:39,786 - scikit_build_core - WARNING - libdir/ldlibrary: /opt/conda/lib/libpython3.10.a is not a real file!\n  2024-01-29 20:55:39,786 - scikit_build_core - WARNING - Can't find a Python library, got libdir=/opt/conda/lib, ldlibrary=libpython3.10.a, multiarch=x86_64-linux-gnu, masd=None\n  /usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n  loading initial cache file /tmp/tmp01qzq97x/build/CMakeInit.txt\n  -- The C compiler identification is GNU 11.4.0\n  -- The CXX compiler identification is GNU 11.4.0\n  -- Detecting C compiler ABI info\n  -- Detecting C compiler ABI info - done\n  -- Check for working C compiler: /usr/bin/cc - skipped\n  -- Detecting C compile features\n  -- Detecting C compile features - done\n  -- Detecting CXX compiler ABI info\n  -- Detecting CXX compiler ABI info - done\n  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n  -- Detecting CXX compile features\n  -- Detecting CXX compile features - done\n  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n  -- Looking for pthread.h\n  -- Looking for pthread.h - found\n  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n  -- Found Threads: TRUE\n  -- Warning: ccache not found - consider installing it or use LLAMA_CCACHE=OFF\n  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n  -- x86 detected\n  \u001b[0mINSTALL TARGETS - target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\u001b[0m\n  \u001b[0mINSTALL TARGETS - target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\u001b[0m\n  -- Configuring done\n  -- Generating done\n  -- Build files have been written to: /tmp/tmp01qzq97x/build\n  \u001b[92m***\u001b[0m \u001b[1mBuilding project with \u001b[94mNinja\u001b[0m...\u001b[0m\n  /usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n  [1/22] cd /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp && /usr/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=11.4.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n  /usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n  [2/22] /usr/bin/c++ -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -std=gnu++11 -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/build-info.cpp\n  [3/22] /usr/bin/cc -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -std=gnu11 -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/ggml-alloc.c\n  [4/22] /usr/bin/cc -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -std=gnu11 -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/ggml-backend.c\n  [5/22] /usr/bin/c++ -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -std=gnu++11 -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/console.cpp\n  [6/22] /usr/bin/c++ -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -std=gnu++11 -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/sampling.cpp\n  [7/22] /usr/bin/c++ -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -std=gnu++11 -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/grammar-parser.cpp\n  [8/22] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/examples/llava/llava.cpp\n  [9/22] /usr/bin/c++ -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -std=gnu++11 -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/train.cpp\n  [10/22] /usr/bin/cc -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -std=gnu11 -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/ggml-quants.c\n  [11/22] /usr/bin/c++  -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/examples/llava/llava-cli.cpp\n  [12/22] /usr/bin/c++ -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -std=gnu++11 -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/common/common.cpp\n  [13/22] /usr/bin/cc -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -std=gnu11 -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/ggml.c\n  [14/22] : && /usr/bin/cmake -E rm -f vendor/llama.cpp/libggml_static.a && /usr/bin/ar qc vendor/llama.cpp/libggml_static.a  vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o && /usr/bin/ranlib vendor/llama.cpp/libggml_static.a && :\n  /usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n  [15/22] : && /usr/bin/cc -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libggml_shared.so -o vendor/llama.cpp/libggml_shared.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o   && :\n  [16/22] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/examples/llava/clip.cpp\n  [17/22] : && /usr/bin/cmake -E rm -f vendor/llama.cpp/examples/llava/libllava_static.a && /usr/bin/ar qc vendor/llama.cpp/examples/llava/libllava_static.a  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o && /usr/bin/ranlib vendor/llama.cpp/examples/llava/libllava_static.a && :\n  /usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n  [18/22] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/. -O3 -DNDEBUG -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -std=gnu++11 -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/vendor/llama.cpp/llama.cpp\n  [19/22] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o vendor/llama.cpp/libllama.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o   && :\n  [20/22] : && /usr/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o && /usr/bin/ranlib vendor/llama.cpp/common/libcommon.a && :\n  /usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n  [21/22] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllava.so -o vendor/llama.cpp/examples/llava/libllava.so vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o  -Wl,-rpath,/tmp/tmp01qzq97x/build/vendor/llama.cpp:  vendor/llama.cpp/libllama.so && :\n  [22/22] : && /usr/bin/c++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -o vendor/llama.cpp/examples/llava/llava-cli  -Wl,-rpath,/tmp/tmp01qzq97x/build/vendor/llama.cpp:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/libllama.so && :\n  \u001b[92m***\u001b[0m \u001b[1mInstalling project into wheel...\u001b[0m\n  /usr/bin/cmake: /opt/conda/lib/libcurl.so.4: no version information available (required by /usr/bin/cmake)\n  -- Install configuration: \"Release\"\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/lib/libggml_shared.so\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/include/ggml.h\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/include/ggml-alloc.h\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/include/ggml-backend.h\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/lib/libllama.so\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/include/llama.h\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/bin/convert.py\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/bin/convert-lora-to-ggml.py\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/llama_cpp/libllama.so\n  -- Installing: /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/llama_cpp/libllama.so\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/lib/libllava.so\n  -- Set runtime path of \"/tmp/tmp01qzq97x/wheel/platlib/lib/libllava.so\" to \"\"\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/bin/llava-cli\n  -- Set runtime path of \"/tmp/tmp01qzq97x/wheel/platlib/bin/llava-cli\" to \"\"\n  -- Installing: /tmp/tmp01qzq97x/wheel/platlib/llama_cpp/libllava.so\n  -- Set runtime path of \"/tmp/tmp01qzq97x/wheel/platlib/llama_cpp/libllava.so\" to \"\"\n  -- Installing: /tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/llama_cpp/libllava.so\n  -- Set runtime path of \"/tmp/pip-install-e3kzm6ja/llama-cpp-python_7c4dcef0caf6472e9b28ba716be897a2/llama_cpp/libllava.so\" to \"\"\n  \u001b[92m***\u001b[0m \u001b[1mMaking wheel...\u001b[0m\n  \u001b[92m***\u001b[0m \u001b[1mCreated\u001b[22m llama_cpp_python-0.2.36-cp310-cp310-manylinux_2_35_x86_64.whl...\u001b[0m\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.36-cp310-cp310-manylinux_2_35_x86_64.whl size=2455716 sha256=ced2b247172da22800030dfe249c965ce48a731b8116b982433374542bd7612d\n  Stored in directory: /root/.cache/pip/wheels/26/0e/4a/94c586df881726eea841c3780236351cdad7bef2303c7607a7\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.36\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Se instala LangChain, tambien te podes loguear para acceder desde tu perfil, pero es opccional","metadata":{}},{"cell_type":"code","source":"# LangChain\n!pip install langchain","metadata":{"execution":{"iopub.status.busy":"2024-01-29T21:00:02.625364Z","iopub.execute_input":"2024-01-29T21:00:02.626283Z","iopub.status.idle":"2024-01-29T21:00:32.384585Z","shell.execute_reply.started":"2024-01-29T21:00:02.626243Z","shell.execute_reply":"2024-01-29T21:00:32.383486Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/23/9f/a78357793c96ae5b53b5a31a891ed2fe3b02dc1a11a705dd14da67932c42/langchain-0.1.4-py3-none-any.whl.metadata\n  Downloading langchain-0.1.4-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.20)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.3)\nCollecting jsonpatch<2.0,>=1.33 (from langchain)\n  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nCollecting langchain-community<0.1,>=0.0.14 (from langchain)\n  Obtaining dependency information for langchain-community<0.1,>=0.0.14 from https://files.pythonhosted.org/packages/57/00/a798f8124db57eb9e20fe31dc7561e15e9c4607281cddaa4db49f93d7111/langchain_community-0.0.16-py3-none-any.whl.metadata\n  Downloading langchain_community-0.0.16-py3-none-any.whl.metadata (7.8 kB)\nCollecting langchain-core<0.2,>=0.1.16 (from langchain)\n  Obtaining dependency information for langchain-core<0.2,>=0.1.16 from https://files.pythonhosted.org/packages/e6/7f/2c5006e2292bbcc9fc7cfaac407954c00fc8c9f5afd4e62c17adc0ba1790/langchain_core-0.1.17-py3-none-any.whl.metadata\n  Downloading langchain_core-0.1.17-py3-none-any.whl.metadata (6.0 kB)\nCollecting langsmith<0.1,>=0.0.83 (from langchain)\n  Obtaining dependency information for langsmith<0.1,>=0.0.83 from https://files.pythonhosted.org/packages/61/f2/45e032811f39cfe79d49935d653f531dbe17c60b30639d4bb6f0cf09d26d/langsmith-0.0.84-py3-none-any.whl.metadata\n  Downloading langsmith-0.0.84-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.3)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.12)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.0)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain) (3.7.1)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.16->langchain)\n  Obtaining dependency information for packaging<24.0,>=23.2 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\n  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.5.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.1.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nDownloading langchain-0.1.4-py3-none-any.whl (803 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading langchain_community-0.0.16-py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.17-py3-none-any.whl (235 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.0.84-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hUsing cached packaging-23.2-py3-none-any.whl (53 kB)\nInstalling collected packages: packaging, jsonpatch, langsmith, langchain-core, langchain-community, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: jsonpatch\n    Found existing installation: jsonpatch 1.32\n    Uninstalling jsonpatch-1.32:\n      Successfully uninstalled jsonpatch-1.32\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.10 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed jsonpatch-1.33 langchain-0.1.4 langchain-community-0.0.16 langchain-core-0.1.17 langsmith-0.0.84 packaging-23.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Aca cargamos el GPU de adorno porque llama-cpp por el motivo x (no hay respuestas, solo preguntas en todos lados) no es compatible con el GPU de kaggle, mas alla de eso la inferencia no es tan lenta, con GPU debe volar!","metadata":{}},{"cell_type":"code","source":"!nvidia-smi\n!nvcc --version\nimport torch\n\nif not torch.cuda.is_available():\n    raise RuntimeError()\n\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_community.llms import LlamaCpp\n\nmodel_path = '/kaggle/working/models--TheBloke--NeuralBeagle14-7B-GGUF/snapshots/8952d8c1de658040fab3a45c9af426d468cfadb2/neuralbeagle14-7b.Q4_0.gguf'  # Reemplaza esto con la ruta de tu modelao\nllm_kwargs = {\n    \"temperature\": 0.75,\n    \"max_tokens\": 5000,\n    \"top_p\": 1,\n    \"n_gpu_layers\": 40,\n    \"n_batch\": 512,\n    \"callback_manager\": CallbackManager([StreamingStdOutCallbackHandler()]),\n    \"verbose\": True,\n}\n\nif not os.path.exists(model_path):\n    raise FileNotFoundError(model_path)\n\nllm = LlamaCpp(\n    model_path=model_path,\n    **llm_kwargs,\n)\n\ntemplate = \"Question: {question}\\n\\nAnswer: Let's work this out in a step by step way to be sure we have the right answer.\"\n\nllm_chain = PromptTemplate(template=template, input_variables=[\"question\"]) | llm\n\nquestion = \"Your question here\"\nanswer = llm_chain.invoke(dict(question=question))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-29T21:05:45.427461Z","iopub.execute_input":"2024-01-29T21:05:45.428290Z","iopub.status.idle":"2024-01-29T21:06:57.054134Z","shell.execute_reply.started":"2024-01-29T21:05:45.428257Z","shell.execute_reply":"2024-01-29T21:06:57.053105Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Mon Jan 29 21:05:46 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P8              10W /  70W |      3MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   40C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\n","output_type":"stream"},{"name":"stderr","text":"llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /kaggle/working/models--TheBloke--NeuralBeagle14-7B-GGUF/snapshots/8952d8c1de658040fab3a45c9af426d468cfadb2/neuralbeagle14-7b.Q4_0.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = mlabonne_neuralbeagle14-7b\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 2\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\nllama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\nllama_model_loader: - kv  21:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q4_0\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \nllm_load_print_meta: general.name     = mlabonne_neuralbeagle14-7b\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 2 '</s>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.11 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =  3917.87 MiB\n...................................................................................................\nllama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\nllama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\nllama_new_context_with_model:        CPU input buffer size   =     9.01 MiB\nllama_new_context_with_model:        CPU compute buffer size =    79.20 MiB\nllama_new_context_with_model: graph splits (measure): 1\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \nModel metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mlabonne_neuralbeagle14-7b', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n","output_type":"stream"},{"name":"stdout","text":" First, let's make sure we understand that 70% of a number is equal to 56. This can be expressed as an equation where x is the unknown number:\n\n0.7x = 56\n\nTo solve for x, we divide both sides of the equation by 0.7 (which is equivalent to multiplying both sides by its reciprocal, which is 1/0.7 or approximately 1.428):\n\nx = 56 / 0.7\nx = 80\n\nThus, if 70% of a number is equal to 56, the actual number is 80.","output_type":"stream"},{"name":"stderr","text":"\nllama_print_timings:        load time =   10407.97 ms\nllama_print_timings:      sample time =      75.05 ms /   146 runs   (    0.51 ms per token,  1945.42 tokens per second)\nllama_print_timings: prompt eval time =   10407.85 ms /    32 tokens (  325.25 ms per token,     3.07 tokens per second)\nllama_print_timings:        eval time =   58076.87 ms /   145 runs   (  400.53 ms per token,     2.50 tokens per second)\nllama_print_timings:       total time =   69306.72 ms /   177 tokens\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Reduccion de código para hacer la inferenccia","metadata":{}},{"cell_type":"code","source":"template = \"Question: {question}\\n\\nAnswer: Quiero que hables siempre en español, y respondas como un asistente\"\n\nllm_chain = PromptTemplate(template=template, input_variables=[\"question\"]) | llm\n\nquestion = \"como crees que va a evolucionas los grandes modelos de lenguajes de la IA?\"\nanswer = llm_chain.invoke(dict(question=question))","metadata":{"execution":{"iopub.status.busy":"2024-01-29T21:32:26.651655Z","iopub.execute_input":"2024-01-29T21:32:26.652584Z","iopub.status.idle":"2024-01-29T21:35:37.488268Z","shell.execute_reply.started":"2024-01-29T21:32:26.652554Z","shell.execute_reply":"2024-01-29T21:35:37.487284Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Llama.generate: prefix-match hit\n","output_type":"stream"},{"name":"stdout","text":" generalizado. A medida que avanza la investigación en inteligencia artificial (IA) y aprendizaje automático, se espera que los grandes modelos de lenguajes de IA sigan evolucionando en varias áreas importantes:\n\n1. **Capacidad**: Actualmente, los modelos más grandes como GPT-3 y BERT tienen capacidades impresionantes pero aún no alcanzan el nivel de un humano en muchas tareas. Se espera que los modelos futuros sean capaces de procesar y entender información con mayor velocidad, precisión y eficiencia.\n\n2. **Especialización**: Actualmente, los modelos de lenguaje se entrenan sobre un conjunto amplio de datos, lo que significa que no son especialistas en un área específica. Se espera que los futuros modelos estén diseñados para ser especialistas en áreas concretas, como medicina, ingeniería, financiero, etc., y puedan proporcionar respuestas más acertadas y detalladas en esos campos.\n\n3. **Ethics, Bias and Privacy**: Actualmente, existen preocupaciones sobre la ética, sesgos y privacidad asociados con los modelos de lenguaje. Se espera que las próximas generaciones de modelos estén diseñadas teniendo en cuenta estos problemas, proporcionando respuestas más responsables, equitativas y respetuosas de la privacidad.\n\n4. **Interoperabilidad**: Actualmente, los modelos de lenguaje están desarrollados por diferentes entidades e investigadores. Se espera que en el futuro se trabaje hacia una mejor interoperabilidad entre estos modelos, permitiendo la integración y combinación de funciones y capacidades de distintos modelos para resolver problemas más complejos.\n\n5.","output_type":"stream"},{"name":"stderr","text":"\nllama_print_timings:        load time =   10407.97 ms\nllama_print_timings:      sample time =     264.07 ms /   459 runs   (    0.58 ms per token,  1738.20 tokens per second)\nllama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:        eval time =  187922.16 ms /   459 runs   (  409.42 ms per token,     2.44 tokens per second)\nllama_print_timings:       total time =  190827.09 ms /   460 tokens\n","output_type":"stream"}]}]}